{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: DSP_NOTEBOOK_CACHEDIR=cache\n"
     ]
    }
   ],
   "source": [
    "%set_env DSP_NOTEBOOK_CACHEDIR cache\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dsp\n",
    "from utils import get_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"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\" #os.getenv('OPENAI_API_KEY')  # or replace with your API key (optional)\n",
    "colbert_server = 'http://ec2-44-228-128-229.us-west-2.compute.amazonaws.com:8893/api/search'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(openai_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = dsp.GPT3(model='text-davinci-002', api_key=openai_key)\n",
    "# rm = dsp.ColBERTv2(url=colbert_server)\n",
    "rm = get_retriever()\n",
    "dsp.settings.configure(lm=lm, rm=rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [('Who produced the album that included a re-recording of \"Lithium\"?', ['Butch Vig']),\n",
    "         ('Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?', ['Kevin Greutert']),\n",
    "         ('The heir to the Du Pont family fortune sponsored what wrestling team?', ['Foxcatcher', 'Team Foxcatcher', 'Foxcatcher Team']),\n",
    "         ('In what year was the star of To Hell and Back born?', ['1925']),\n",
    "         ('Which award did the first book of Gary Zukav receive?', ['U.S. National Book Award', 'National Book Award']),\n",
    "         ('What city was the victim of Joseph Druces working in?', ['Boston, Massachusetts', 'Boston']),]\n",
    "\n",
    "train = [dsp.Example(question=question, answer=answer) for question, answer in train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = dsp.Type(prefix=\"Question:\", desc=\"${the question to be answered}\")\n",
    "answer = dsp.Type(prefix=\"Answer:\", desc=\"${a short factoid answer, often between 1 and 5 words}\", format=dsp.format_answers)\n",
    "\n",
    "qa_template = dsp.Template(instructions=\"Answer questions with short factoid answers.\", question=question(), answer=answer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dsp.Type(\n",
    "    prefix=\"Context:\\n\",\n",
    "    desc=\"${sources that may contain relevant content}\",\n",
    "    format=dsp.passages2text\n",
    ")\n",
    "\n",
    "qa_template_with_passages = dsp.Template(\n",
    "    instructions=qa_template.instructions,\n",
    "    context=context(), question=question(), answer=answer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program 1: RTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_then_read_QA(question: str) -> str:\n",
    "    demos = dsp.sample(train, k=5)\n",
    "    passages = rm(question, k=10)\n",
    "    \n",
    "    example = dsp.Example(question=question, context=passages, demos=demos)\n",
    "    example, completions = dsp.generate(qa_template_with_passages)(example, stage='qa')\n",
    "\n",
    "    return completions.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which award did the first book of Gary Zukav receive?\n",
      "Answer: U.S. National Book Award\n",
      "\n",
      "Question: The heir to the Du Pont family fortune sponsored what wrestling team?\n",
      "Answer: Foxcatcher\n",
      "\n",
      "Question: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\n",
      "Answer: Kevin Greutert\n",
      "\n",
      "Question: Who produced the album that included a re-recording of \"Lithium\"?\n",
      "Answer: Butch Vig\n",
      "\n",
      "Question: What city was the victim of Joseph Druces working in?\n",
      "Answer: Boston, Massachusetts\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context:\n",
      "${sources that may contain relevant content}\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «1908.10408v1.txt | The Query Encoder is the standard Transformer encoder, whose output token representations are subject to a projection to maintain the same model dimensionality across the levels. The Masked Session Encoder prevents information flow from subsequent queries. The shadow underneath the encoders conveys that they consist of several layers. Note that we add positional encodings (PE) for tokens in each query, and for queries in each session. The Add & Norm layer first computes the sum of its two arguments, one from the masked session encoder and the other via a residual or skip connection from the query encoder, and then performs layer normalization. The output codes thus obtained for each query are subsequently fed into a standard Transformer decoder (not shown in the figure) that treats the queries independently. The decoder of MTN is the same as in Transformer (i.e.»\n",
      "[2] «2111.05937v1.txt | Figure 4: Dual Encoder or Two Tower architecture [42]. Query Encoder (also referred to as Query Tower) is used for encoding the question, Document Encoder (also referred to as Document Tower) is used to encode the documents. The encoders are usually BERT models. In ODQA retrieval models like DPR and ORQA [101, 112], only the [CLS] output vector of the BERT model outputs embedding is used as the embedding vectors and the other token embeddings are ignored. The similarity value is usually the dot product among the normalized vectors.»\n",
      "[3] «2208.07652v1.txt | 3.2.1 Query Encoder. The query encoder is to map the input query q = {w 1, w 2, . . . , w |q | } into a compact vector that can capture its essential topics. Specifically, the encoder represents the query q as a series of hidden vectors, i.e., Hq = Encoder(w 1, w 2, . . . , w |q | ), where Hq denotes the query representation. fi fi fi 3.2.2 Identifier Decoder. The decoder is to generate a sequence of document identifiers of the relevant documents for each query. Note the document identifier can be defined in different ways, such as the unique title and url of the document. Specifically, the probability of generating the n-th token wm,n in the m-th document identifier tm is defined as, p (wm,n |w ≤m,<n , q) = Decoder(w ≤m,<n , Hq ).»\n",
      "[4] «2003.00708v1.txt | Query Encoder: The query encoder generates a query level encoding Vqi for every qi ∈ S. This is done by first representing the query qi using vector embeddings of corresponding words {w1 , . . . , wlq }, and then sequentially feeding them into a bidirectional LSTM (BiLSTM) [14]. As shown in Fig. 2(a), the query encoder takes each of these word representations as input to the BiLSTM at every encoding step and updates the hidden states based on the forward and Image Captions and Multitask Learning for Query Reformulations 5 Updating of the session encoding after seeing query in session max-pooling max-pooling max-pooling k transitions from (i-1) to i t=1 k=1 k=2 k=3 k=lq k=1 k=2 k=i-1 k=1 k=2 k=3 k=i-1 (b) Session Encoder (a) Query Encoder Fig. 2.»\n",
      "[5] «2009.10791v1.txt | USE-QA: The USE-QA retriever (Yang et al., 2019b) has separate encoders for the query and document. The query encoder is a transformer-based model, producing a 512-dimension embedding as the query representation. The document encoder has a transformer-based model to encode the answer sentence and a Convolutional Neural Network (CNN)-based model to encode the context. A single 512-dimension embedding is produced as the document representation. Finally, the relevance score is computed as the dot product of the query embedding and the document embedding. USEQA is pre-trained on large scale retrieval tasks and, as used in (Ahmad et al., 2019), we do not fine-tune it in the retrieval tasks.»\n",
      "\n",
      "Question: Query encoder\n",
      "\n",
      "Answer:\u001b[32m A query encoder is a transformer-based model that produces a 512-dimension embedding as the query representation.\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('A query encoder is a transformer-based model that produces a 512-dimension embedding as the query representation.',\n",
       " None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_then_read_QA(\"Query encoder\"), lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program 2: TOT\n",
    "\n",
    "This program performs the search step to retrieve the most relevant documents from the retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale = dsp.Type(\n",
    "    prefix=\"Rationale: Let's think step by step.\",\n",
    "    desc=\"${a step-by-step deduction that identifies the correct response, which will be provided below}\"\n",
    ")\n",
    "\n",
    "qa_template_with_CoT = dsp.Template(\n",
    "    instructions=qa_template.instructions,\n",
    "    context=context(), question=question(), rationale=rationale(), answer=answer()\n",
    ")\n",
    "\n",
    "search_rationale = dsp.Type(\n",
    "    prefix=\"Rationale: Let's think step by step. To answer this question, we first need to find out\",\n",
    "    desc=\"${the missing information}\"\n",
    ")\n",
    "\n",
    "search_query = dsp.Type(\n",
    "    prefix=\"Search Query:\",\n",
    "    desc=\"${a simple question for seeking the missing information}\"\n",
    ")\n",
    "\n",
    "rewrite_template = dsp.Template(\n",
    "    instructions=\"Write a search query that will help answer a complex question.\",\n",
    "    question=question(), rationale=search_rationale(), query=search_query()\n",
    ")\n",
    "\n",
    "condensed_rationale = dsp.Type(\n",
    "    prefix=\"Rationale: Let's think step by step. Based on the context, we have learned the following.\",\n",
    "    desc=\"${information from the context that provides useful clues}\"\n",
    ")\n",
    "\n",
    "hop_template = dsp.Template(\n",
    "    instructions=rewrite_template.instructions,\n",
    "    context=context(), question=question(), rationale=condensed_rationale(), query=search_query()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dsp.utils import deduplicate\n",
    "\n",
    "@dsp.transformation\n",
    "def qa_predict(example: dsp.Example, sc=True):\n",
    "    if sc:\n",
    "        example, completions = dsp.generate(qa_template_with_CoT, n=20, temperature=0.7)(example, stage='qa')\n",
    "        completions = dsp.majority(completions)\n",
    "    else:\n",
    "        example, completions = dsp.generate(qa_template_with_CoT)(example, stage='qa')\n",
    "    \n",
    "    return example.copy(answer=completions.answer)\n",
    "\n",
    "@dsp.transformation\n",
    "def multihop_search(example: dsp.Example, max_hops=2, k=10) -> dsp.Example:\n",
    "    example.context = []\n",
    "    \n",
    "    for hop in range(max_hops):\n",
    "        # Generate a query based\n",
    "        template = rewrite_template if hop == 0 else hop_template\n",
    "        example, completions = dsp.generate(template)(example, stage=f'h{hop}')\n",
    "\n",
    "        # Retrieve k results based on the query generated\n",
    "        passages = rm(completions.query, k=k)\n",
    "\n",
    "        # Update the context by concatenating old and new passages\n",
    "        example.context = deduplicate(example.context + passages)\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multihop_qa(question: str) -> str:\n",
    "    demos = dsp.sample(train, k=7)\n",
    "    x = dsp.Example(question=question, demos=demos)\n",
    "    \n",
    "    x = multihop_search(x)\n",
    "    x = qa_predict(x, sc=True)\n",
    "\n",
    "    return x.answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Answer questions with short factoid answers.\n",
      "\n",
      "---\n",
      "\n",
      "Question: Which award did the first book of Gary Zukav receive?\n",
      "Answer: U.S. National Book Award\n",
      "\n",
      "Question: The heir to the Du Pont family fortune sponsored what wrestling team?\n",
      "Answer: Foxcatcher\n",
      "\n",
      "Question: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\n",
      "Answer: Kevin Greutert\n",
      "\n",
      "Question: Who produced the album that included a re-recording of \"Lithium\"?\n",
      "Answer: Butch Vig\n",
      "\n",
      "Question: What city was the victim of Joseph Druces working in?\n",
      "Answer: Boston, Massachusetts\n",
      "\n",
      "Question: In what year was the star of To Hell and Back born?\n",
      "Answer: 1925\n",
      "\n",
      "---\n",
      "\n",
      "Follow the following format.\n",
      "\n",
      "Context:\n",
      "${sources that may contain relevant content}\n",
      "\n",
      "Question: ${the question to be answered}\n",
      "\n",
      "Rationale: Let's think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\n",
      "\n",
      "Answer: ${a short factoid answer, often between 1 and 5 words}\n",
      "\n",
      "---\n",
      "\n",
      "Context:\n",
      "[1] «2210.06345v1.txt | Omar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via contextualized late interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 39–48. Association for Computing Machinery, New York, NY, USA, July 2020. ISBN 9781450380164. doi: 10.1145/3397271. 3401075. Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for OpenQA with ColBERT. Transactions of the Association for Computational Linguistics, 9:929–944, September 2021. ISSN 2307-387X. doi: 10.1162/tacl\\_a\\_00405. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are Zero-Shot reasoners. May 2022. Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The Gumbel-Top-k trick for sampling sequences without replacement. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3499–3508. PMLR, 2019a.»\n",
      "[2] «2212.09597v1.txt | (2022) indicates that LMs are also zero-shot reasoners without needing extra exemplars. By only concatenating \"Let's think step by step\", LMs can consciously generate reasoning steps. Multi-Stage. When human beings are reasoning, it is usually challenging to come up with the whole reasoning process in one stroke. A more intuitive solution is to decompose a complex problem into simpler sub-problems and reason stage by stage. Similarly, this series of works aims to transform previous one-stage prompting into multistage prompting. Press et al. (2022) explicitly defines follow-up questions and intermediate answers in prompts to narrow the compositionality gap in LMs. Jung et al. (2022) regard the output of each stage as a separate new question while Zhou et al.»\n",
      "[3] «2212.09597v1.txt | Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. CoRR, abs/2210.02406. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. CoRR, abs/2205.11916. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing Algebraic Word Problems into Equations. Transactions of the Association for Computational Linguistics, 3:585–597. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152–1157, San Diego, California. Association for Computational Linguistics. Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S. Bernstein. 2022. Socially situated artificial intelligence enables learning from human interaction. Proceedings of the National Academy of Sciences, 119(39):e2115730119.»\n",
      "[4] «2210.05075v1.txt | Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1596–1606, 2019. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871–7880, 2020. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.»\n",
      "[5] «2105.08318v2.txt | In this paper, we explore the possibility of zero-shot learning in RecSys, to enable generalization from an old dataset to an entirely new dataset. We develop an algorithm, dubbed ZEro-Shot Recommenders (ZESR EC), that is trained on an old dataset and generalize to a new one where there are neither overlapping users nor overlapping items, a setting that contrasts typical cross-domain RecSys that has either overlapping users or items. Different from previous methods that use categorical item indices (i.e., item ID), ZESR EC uses items' generic features, such as natural-language descriptions, product images, and videos, as their continuous indices, and therefore naturally generalizes to any unseen items. In terms of users, ZESR EC builds upon recent advances on sequential RecSys to represent users using their interactions with items, thereby generalizing to unseen users as well.»\n",
      "[6] «2207.12515v1.txt | • Big Models: Recently, Foundation Models such as Large Language Models (LLM) have achieved surprisingly good performance in many AI sub-fields, which have the advantages of emergent capabilities from model size, extracting useful information based on self-supervision, unifying various downstream tasks based on pre-training, fine-tuning and prompting, as well as generalizing to zero-shot or few-shot cases [31, 290]. Many powerful foundation models have been developed for natural language tasks such as T5 [231], GPT-3 [35], OPT [335] and PaLM [69], which show impressive performance on language understanding, generation and reasoning tasks.»\n",
      "[7] «2206.02873v5.txt | Additionally, larger language models demonstrated stronger zero-shot and few-shot capabilities compared to smaller models. Brown et al. [3] presented GPT-3, a language model with 175 billion parameters. The authors evaluated the model's ability to learn from a few examples and demonstrated that scaling up the number of parameters greatly improves zero-shot and few-shot effectiveness. Wei et al. [44] presented instruction tuning, a novel method to improve the zero-shot learning capacity of scaled language models by providing task-specific instructions using natural language.»\n",
      "[8] «2212.09597v1.txt | 6 Future Directions Even though numerous works have been proposed for reasoning with language model prompting, there remain some potential directions: Theoretical Principle of Reasoning. LMs have been demonstrated to have emergent zero-shot learning and reasoning abilities (Wei et al., 2022b; Wang et al., 2022g; Wei et al., 2022a). To uncover the mystery of such a success, many researchers have empirically explored the role of in-context learning (Ye and Durrett, 2022; Liu et al., 2022a) and rationales (Min et»\n",
      "[9] «2205.08084v2.txt | 4.2 Zero-Shot Recommendation The unique advantage of using a pretrained language model as the foundation is that it can judge the likelihood of any event by expressing the event in natural language. To corroborate this statement, in Figure 7 we verify M6-Rec's ability to perform zeroshot ranking on three datasets of different domains, using the zeroshot method described in Subsection 3.2. Moreover, after fitting the language loss on a few samples, M6-Rec can match the performance of a traditional ID-based ranker trained on a million samples.»\n",
      "\n",
      "Question: Large Language Models are zero-shot reasoners\n",
      "\n",
      "Rationale: Let's think step by step.\u001b[32m The question is asking if Large Language Models are zero-shot reasoners. To answer this, we need to find a source that discusses Large Language Models and zero-shot reasoning.\n",
      "\n",
      "Answer: Yes\u001b[0m\u001b[31m \t (and 19 other completions)\u001b[0m\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Yes', None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multihop_qa(\"Large Language Models are zero-shot reasoners\"), lm.inspect_history(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "router_template = \"\"\"Classify a sentence as RTR, TOT, or \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit ('3.10.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0179502f83bb99383e222571be9e072435f38c9dbfbe673b31be98361f58e1a4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
