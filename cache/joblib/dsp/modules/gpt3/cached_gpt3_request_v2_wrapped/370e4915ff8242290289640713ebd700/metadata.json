{"duration": 4.4366350173950195, "input_args": {"**": "{'frequency_penalty': 0, 'logprobs': 5, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 20, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\nQuestion: In what year was the star of To Hell and Back born?\\nAnswer: 1925\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab1904.05880v3.txt | Zitnick, and D. Parikh. VQA: Visual question answering. In ICCV, 2015. 1, 2, 8 [6] H. Ben-Younes, R. Cadene, M. Cord, and N. Thome. Mutan: Multimodal tucker fusion for visual question answering. In ICCV, 2017. 2 [7] M. Chatterjee and A. G. Schwing. Diverse and Coherent Paragraph Generation from Images. In Proc. ECCV, 2018. 2 [8] K. Clark and C. Manning. Deep reinforcement learning for mention-ranking coreference models. arXiv preprint arXiv:1609.08667, 2016. 2 [9] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra.\u00bb\\n[2] \u00ab1911.05161v1.txt | CoDS-COMAD \\'20, January 05\u201307, 2020, Hyderabad, India [11] Julian Kupiec. 1993. MURAX: A Robust Linguistic Approach for Question Answering Using an On-line Encyclopedia. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR \\'93). ACM, New York, NY, USA, 181\u2013190. https://doi.org/10. 1145/160688.160717 [12] Bernardo Magnini, Matteo Negri, Roberto Prevete, and Hristo Tanev. 2002. Is It the Right Answer?: Exploiting Web Redundancy for Answer Validation.\u00bb\\n[3] \u00ab1905.10718v1.txt | We also compare HAS with other baselines in existing works. KAN and MULT do question-answer interaction before encoding layer or during encoding layer, and the outputs of the encoding layer for an answer are different for different questions. Thus, these two models cannot store representations for reusing. We compare HAS with Multihop-Sequential-LSTM, AP-CNN, and AP-BiLSTM. The memory cost of these three models is 5.25 GB, 7.44 GB, and 5.25 GB, respectively, which are 11.75, 16.67, 11.75 times larger than that of HAS. Other baselines are not adopted for comparison, but almost all baselines with question-answer interaction mechanisms have either time cost problem or memory cost problem as that in BERT-attention. We can find that our HAS is fast with a low memory cost, which also makes HAS have promising potential for embedded or mobile applications.\u00bb\\n[4] \u00ab2005.01218v1.txt | Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results\u00bb\\n[5] \u00ab2008.02434v1.txt | Conference \\'20, October, 2020, New York, USA 6 CONCLUSIONS In this paper, we present a system MurKe that answers healthcare exam questions by using knowledge extraction and multi-step reasoning. To get a relevant document for each question, MurKe retrieves supporting documents from a large, noisy corpus on the basis of keywords extracted from the original question and semantic retrieval. MurKe proposes the multi-step iterative method to solve complex healthcare QA, which uses information selected by combining iterative question reformulation and textual entailment. Our neural architecture uses a sequence of token-level attention mechanisms to extract relevant evidence from the selected documents in order to update the latent representation of the question, which shows the interpretability of the reasoning path. Through empirical results and case study, we demonstrate that our proposed system is able to outperform several strong baselines on the HeadQA dataset.\u00bb\\n[6] \u00ab2111.05937v1.txt | In [185], the authors describe a system named QAmp for answering complex multihop questions. The method breaks down the task of query formation into two steps. In the question interpretation step, the entity, relation and class keywords are first identified from the question surface form using a BiLSTM-CRF [84] model, followed by a reranking step which ranks different terms from the KG matching the identified question surface terms. Several potential query subgraphs are constructed using message passing methodology, and executed to produce several probable answers. The answers are ranked using the aggregated reranking score of different components of the subgraph. Additionally, a prediction of the type of the question (select, count, ask) is made, which helps derive the final answer.\u00bb\\n\\nQuestion: Mutihop Question Answering\\n\\nRationale: Let\\'s think step by step.', 'temperature': 0.7, 'top_p': 1}"}}