{"duration": 1.2976088523864746, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab1904.05880v3.txt | Zitnick, and D. Parikh. VQA: Visual question answering. In ICCV, 2015. 1, 2, 8 [6] H. Ben-Younes, R. Cadene, M. Cord, and N. Thome. Mutan: Multimodal tucker fusion for visual question answering. In ICCV, 2017. 2 [7] M. Chatterjee and A. G. Schwing. Diverse and Coherent Paragraph Generation from Images. In Proc. ECCV, 2018. 2 [8] K. Clark and C. Manning. Deep reinforcement learning for mention-ranking coreference models. arXiv preprint arXiv:1609.08667, 2016. 2 [9] A. Das, H. Agrawal, C. L. Zitnick, D. Parikh, and D. Batra.\u00bb\\n[2] \u00ab1911.05161v1.txt | CoDS-COMAD '20, January 05\u201307, 2020, Hyderabad, India [11] Julian Kupiec. 1993. MURAX: A Robust Linguistic Approach for Question Answering Using an On-line Encyclopedia. In Proceedings of the 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR '93). ACM, New York, NY, USA, 181\u2013190. https://doi.org/10. 1145/160688.160717 [12] Bernardo Magnini, Matteo Negri, Roberto Prevete, and Hristo Tanev. 2002. Is It the Right Answer?: Exploiting Web Redundancy for Answer Validation.\u00bb\\n[3] \u00ab1905.10718v1.txt | We also compare HAS with other baselines in existing works. KAN and MULT do question-answer interaction before encoding layer or during encoding layer, and the outputs of the encoding layer for an answer are different for different questions. Thus, these two models cannot store representations for reusing. We compare HAS with Multihop-Sequential-LSTM, AP-CNN, and AP-BiLSTM. The memory cost of these three models is 5.25 GB, 7.44 GB, and 5.25 GB, respectively, which are 11.75, 16.67, 11.75 times larger than that of HAS. Other baselines are not adopted for comparison, but almost all baselines with question-answer interaction mechanisms have either time cost problem or memory cost problem as that in BERT-attention. We can find that our HAS is fast with a low memory cost, which also makes HAS have promising potential for embedded or mobile applications.\u00bb\\n[4] \u00ab2005.01218v1.txt | Question answering (QA) is one of the challenging natural language processing (NLP) tasks that benefits from explainability. In particular, multihop QA requires the aggregation of multiple evidence facts in order to answer complex natural language questions (Yang et al., 2018). Several multi-hop QA datasets have been proposed recently (Yang et al., 2018; Khashabi et al., 2018a; Welbl et al., 2018; Dua et al., 2019; Chen and Durrett, 2019; Khot et al., 2019a; Sun et al., 2019b; Jansen and Ustalov, 2019; Rajpurkar et al., 2018). While several neural methods have achieved state-of-theart results\u00bb\\n[5] \u00ab2008.02434v1.txt | Conference '20, October, 2020, New York, USA 6 CONCLUSIONS In this paper, we present a system MurKe that answers healthcare exam questions by using knowledge extraction and multi-step reasoning. To get a relevant document for each question, MurKe retrieves supporting documents from a large, noisy corpus on the basis of keywords extracted from the original question and semantic retrieval. MurKe proposes the multi-step iterative method to solve complex healthcare QA, which uses information selected by combining iterative question reformulation and textual entailment. Our neural architecture uses a sequence of token-level attention mechanisms to extract relevant evidence from the selected documents in order to update the latent representation of the question, which shows the interpretability of the reasoning path. Through empirical results and case study, we demonstrate that our proposed system is able to outperform several strong baselines on the HeadQA dataset.\u00bb\\n\\nQuestion: Mutihop Question Answering\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}