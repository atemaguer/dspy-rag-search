{"duration": 2.580618143081665, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2210.06345v1.txt | Omar Khattab and Matei Zaharia. ColBERT: Efficient and effective passage search via contextualized late interaction over BERT. In Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 39\u201348. Association for Computing Machinery, New York, NY, USA, July 2020. ISBN 9781450380164. doi: 10.1145/3397271. 3401075. Omar Khattab, Christopher Potts, and Matei Zaharia. Relevance-guided supervision for OpenQA with ColBERT. Transactions of the Association for Computational Linguistics, 9:929\u2013944, September 2021. ISSN 2307-387X. doi: 10.1162/tacl\\\\_a\\\\_00405. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are Zero-Shot reasoners. May 2022. Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The Gumbel-Top-k trick for sampling sequences without replacement. In Kamalika Chaudhuri and Ruslan Salakhutdinov (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 3499\u20133508. PMLR, 2019a.\u00bb\\n[2] \u00ab2212.09597v1.txt | (2022) indicates that LMs are also zero-shot reasoners without needing extra exemplars. By only concatenating \"Let\\'s think step by step\", LMs can consciously generate reasoning steps. Multi-Stage. When human beings are reasoning, it is usually challenging to come up with the whole reasoning process in one stroke. A more intuitive solution is to decompose a complex problem into simpler sub-problems and reason stage by stage. Similarly, this series of works aims to transform previous one-stage prompting into multistage prompting. Press et al. (2022) explicitly defines follow-up questions and intermediate answers in prompts to narrow the compositionality gap in LMs. Jung et al. (2022) regard the output of each stage as a separate new question while Zhou et al.\u00bb\\n[3] \u00ab2212.09597v1.txt | Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. CoRR, abs/2210.02406. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. CoRR, abs/2205.11916. Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni, and Siena Dumas Ang. 2015. Parsing Algebraic Word Problems into Equations. Transactions of the Association for Computational Linguistics, 3:585\u2013597. Rik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate Kushman, and Hannaneh Hajishirzi. 2016. MAWPS: A math word problem repository. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1152\u20131157, San Diego, California. Association for Computational Linguistics. Ranjay Krishna, Donsuk Lee, Li Fei-Fei, and Michael S. Bernstein. 2022. Socially situated artificial intelligence enables learning from human interaction. Proceedings of the National Academy of Sciences, 119(39):e2115730119.\u00bb\\n[4] \u00ab2210.05075v1.txt | Minghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1596\u20131606, 2019. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 7871\u20137880, 2020. Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. Solving quantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.\u00bb\\n[5] \u00ab2105.08318v2.txt | In this paper, we explore the possibility of zero-shot learning in RecSys, to enable generalization from an old dataset to an entirely new dataset. We develop an algorithm, dubbed ZEro-Shot Recommenders (ZESR EC), that is trained on an old dataset and generalize to a new one where there are neither overlapping users nor overlapping items, a setting that contrasts typical cross-domain RecSys that has either overlapping users or items. Different from previous methods that use categorical item indices (i.e., item ID), ZESR EC uses items\\' generic features, such as natural-language descriptions, product images, and videos, as their continuous indices, and therefore naturally generalizes to any unseen items. In terms of users, ZESR EC builds upon recent advances on sequential RecSys to represent users using their interactions with items, thereby generalizing to unseen users as well.\u00bb\\n\\nQuestion: Large Language Models are zero-shot reasoners\\n\\nRationale: Let\\'s think step by step. Based on the context, we have learned the following.', 'temperature': 0.0, 'top_p': 1}"}}