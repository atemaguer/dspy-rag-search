{"duration": 2.443979024887085, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2207.12515v1.txt | \u2022 Big Models: Recently, Foundation Models such as Large Language Models (LLM) have achieved surprisingly good performance in many AI sub-fields, which have the advantages of emergent capabilities from model size, extracting useful information based on self-supervision, unifying various downstream tasks based on pre-training, fine-tuning and prompting, as well as generalizing to zero-shot or few-shot cases [31, 290]. Many powerful foundation models have been developed for natural language tasks such as T5 [231], GPT-3 [35], OPT [335] and PaLM [69], which show impressive performance on language understanding, generation and reasoning tasks.\u00bb\\n[2] \u00ab2212.09597v1.txt | Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. CoRR, abs/2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, December 6-14, 2022, virtual. Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. 2022c. PINTO: faithful language reasoning using prompt-generated rationales. CoRR, abs/2211.01562.\u00bb\\n[3] \u00ab2212.09597v1.txt | 6 Future Directions Even though numerous works have been proposed for reasoning with language model prompting, there remain some potential directions: Theoretical Principle of Reasoning. LMs have been demonstrated to have emergent zero-shot learning and reasoning abilities (Wei et al., 2022b; Wang et al., 2022g; Wei et al., 2022a). To uncover the mystery of such a success, many researchers have empirically explored the role of in-context learning (Ye and Durrett, 2022; Liu et al., 2022a) and rationales (Min et\u00bb\\n[4] \u00ab2210.05075v1.txt | Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 38\u201345, 2020.\u00bb\\n[5] \u00ab2207.12515v1.txt | 2013. Scrutable user models and personalised item recommendation in mobile lifestyle applications. In International Conference on User Modeling, Adaptation, and Personalization. Springer, 77\u201388. [290] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022. Emergent Abilities of Large Language Models. arXiv preprint arXiv:2206.07682 (2022). [291] Gerhard Weikum, Xin Luna Dong, Simon Razniewski, Fabian Suchanek, et al. 2021. Machine knowledge: Creation and curation of comprehensive knowledge bases. Foundations and Trends\u00ae in Databases 10, 2-4 (2021), 108\u2013490. [292] Udi Weinsberg, Smriti Bhagat, Stratis Ioannidis, and Nina Taft. 2012. s. In Proceedings of the sixth ACM conference on Recommender systems. 195\u2013202. [293] Chad Williams and Bamshad Mobasher. 2006. Profile injection attack detection for securing collaborative recommender systems. DePaul University CTI Technical Report (2006), 1\u201347.\u00bb\\n\\nQuestion: Emergent Abilities in Language Models\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}