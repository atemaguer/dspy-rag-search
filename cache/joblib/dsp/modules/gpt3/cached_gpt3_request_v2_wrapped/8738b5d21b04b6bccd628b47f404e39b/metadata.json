{"duration": 1.4343130588531494, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab1904.07531v4.txt | Large Capacity. BERT uses standard Transformer architecture- multi-head attentions between all term pairs in the text sequence- but makes it very deep. Its main version, BERT-Large, includes 24 Transformer layers, each with 1024 hidden dimensions and 16 attention heads. It in total has 340 million learned parameters, much bigger than typical Neu-IR models. Pretraining. BERT learns from the surrounding context signals in Google News and Wikipedia corpora. It is learned using two tasks: the first predicts random missing words (15%) using the rest of the sentence (Mask-LM); the second predicts whether two sentences appear next to each other. In the second task, the two sentences are concatenated to one sequence; a special token \"[SEP]\" marks the sequence boundaries. Its deep network is very resource consuming in training: BERT-Large takes four days to train on 64 TPUs and easily takes months on typical GPUs clusters.\u00bb\\n[2] \u00ab2012.02287v1.txt | Not including the CNN bias weights, this results in 9,954,000 parameters, or roughly 120 million when also including BERT BASE . The parameter count for RAG is from the original publication, with 220 million parameters from the two BERT BASE models of the biencoder retriever and 406 million parameters from the seq2seq BART LARGE model used for generation (classification in the case of FEVER). The estimates for C OMPOUND L ABEL and NSMN are from Nie et al. (2020). The BERT LARGE +KB FEAT model includes BERT LARGE , a separate Named Entity Recognition model, and a separate textual entailment model, so it has at least the 340 million parameters of BERT LARGE .\u00bb\\n[3] \u00ab2012.02287v1.txt | In particular, BERT LARGE +FT is much stronger than a random baseline, which would be around 33.00, since the dataset is balanced. Interestingly, by adding around 10 million parameters to the smaller BERT BASE model with our proposed approach with supervised evidence training, we can dramatically improve effectiveness. It is conceivable that LM-only effectiveness will increase with increasing parameters; however, at the scale ex- 3 5 30 100 67.67 69.33 73.04 73.49 (equivalent to z = 3) 69.39 55.17 73.07 65.93 73.59 67.70 50.71 54.73 65.27 66.85 Table 2: Dev. set accuracy (Acc.) and FEVER score (FEV.) vs. level 1 beam size (k1 ) and the number of evidence sentences marginalized in level 3 (z) for BERT BASE +M EM M ATCH. amined here, the difference between explicitly retrieving Wikipedia sentences and only relying on LM parameters, which indirectly learn Wikipedia from pre-training, is stark.\u00bb\\n[4] \u00ab2004.06774v4.txt | \u2022 Embedding dimension: 300 \u2022 Minimal number of word occurrences: 3 \u2022 Number of epochs: 50 B.3 Transformer Models\\' Parameters Below we list the hyperparameters that we used for training across all transformer based models. All experimental scripts will be publicly Hyper-parameters include: \u2022 Batch size: 8 \u2022 Number of epochs: 10 \u2022 Max seq length: 128 \u2022 Learning rate (Adam): 2e-5 Number of parameters: \u2022 BERT (bert-base-uncased): L=12, H=768, A=12, total parameters = 110M; where L is number of layers (i.e., Transformer blocks), H is the hidden size, and A is the number of self-attention heads. \u2022 DistilBERT (distilbert-base-uncased): it is a distilled version of the BERT model consists of 6-layer, 768-hidden, 12-heads, 66M parameters. \u2022 RoBERTa (roberta-large): it is using the BERT-large architecture consists of 24-layer, 1024-hidden, 16-heads, 355M parameters.\u00bb\\n[5] \u00ab2012.06690v1.txt | Two BERT architectures are available: one smaller model called base BERT with 110M parameters (12 transformer blocks, 768 hidden size, 12 self-attention heads), and another larger model called large BERT with 340M parameters (24 transformer blocks, 1024 hidden size, 16 self-attention heads). Both of them can be uncased and cased, depending on the pretraining English text. We use bert-base-uncased, bert-base-cased, and bert-large-cased, and their results on the validation set are shown in the Table 6. It can be found that the cased, large models outperform the uncased, based ones. This conforms to the usual expectations, since upper-case words could have different meanings than lower-case ones, and lager model has more parameters to fit the data. However, large BERT model only gives 0.31% increase in accuracy with double running time. To keep a reasonable running time on Google Colab, we will use base architectures for other transformer-based models.\u00bb\\n\\nQuestion: BERT-large parameters\\n\\nAnswer:', 'temperature': 0.0, 'top_p': 1}"}}