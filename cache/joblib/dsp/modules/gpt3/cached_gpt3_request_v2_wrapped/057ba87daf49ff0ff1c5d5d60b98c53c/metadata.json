{"duration": 1.1739692687988281, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab2102.03062v3.txt | OpenAI has set up an application programming interface (API) where users access GPT-3 and \"program\" it by entering words in human language, giving instructions on what to do, and, optionally, a few examples of the desired output. That way, GPT-3 gets a similar number of instructions to what humans would get when asked to perform a task (unlike older NLP models, which require large sets of training data), providing the flexibility needed for a wide range of applications. As such a general-purpose language model, GPT-3 can be applied to a wide variety of tasks. GPT-3 has proven capable of generating poetry or articles indistinguishable from human authors, computer code for web interfaces, and product or job descriptions (Dale, 2020).\u00bb\\n[2] \u00ab2206.12449v1.txt | 5.2.3 Implicit Knowledge Source. In our implementation, we regard GPT-3 [3] as an implicit knowledge source (Figure 7c), which can be replaced by other large-scale pre-trained language models. GPT-3 is proven to have the ability of in-context learning, which means it can be quickly adapted to new tasks with only a few examples in the inference phrase without fine-tuning. As shown in Figure 8, we propose two types of approaches to obtaining implicit knowledge from GPT-3.\u00bb\\n[3] \u00ab2206.12449v1.txt | The first method is to use GPT-3 as a policy model to generate implicit knowledge. We inputted two sampled dialogs as in-context learning examples, concatenated with current dialog history, and the generated system response is used as implicit knowledge. GPT-3 is expected to learn from in-context learning examples and, therefore, generate a reasonable response to the current user utterance. The in-context example was in the form of <user utterance>\\\\n<system utterance>\\\\n* * * <system utterance>\\\\n\\\\n, where \\\\n is used as in-dialog separator and \\\\n\\\\n is used to separate different sample dialogs. The upper part of Figure 5 shows the example of implicit knowledge generation by using GPT-3 as a policy model based on dialog history for the unanswerable question shown in Figure 4.\u00bb\\n[4] \u00ab2102.03062v3.txt | GPT-3 Is a Few-Shot-Learner Another limitation relates to the fact that, as previously mentioned, GPT-3 works without large fine-tuning data sets. By its nature, the language model has knowledge about almost any domain. It learns based on only a few commands on what to do and ideally, some examples of the desired output (few-shot learning) (Wang et al., 2020). We describe above how GPT-3 therefore performs well in tasks that require only general information but struggles with tasks that require further knowledge. However, when writing emails in a work environment (which may include a large portion of emails), it is often necessary to use just such (business-internal) information that GPT-3 has never seen before. In the following, we propose a solution that allows us to augment GPT-3 with business-internal information.\u00bb\\n[5] \u00ab2102.03062v3.txt | By all indications, the quality of GPT-3 generated text is many ways similar to that of text written by humans. GPT-3 can generate grammatically correct, coherent responses. With its vast, previous knowledge, it can even answer emails where the message is not completely specified, like open-ended questions (Dale, 2020). This means that, for example, if a user asks 7 GPT-3 for a nice restaurant in a given city, it will be able to recommend a place to eat. Even more remarkably, it can do so in a conversational manner and mimic empathy (Aronsson et al., 2020), allowing the email recipient to feel understood.\u00bb\\n\\nQuestion: GPT-3\\n\\nAnswer:', 'temperature': 0.0, 'top_p': 1}"}}