{"duration": 1.3940820693969727, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2110.11570v2.txt | Contrastive Learning Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. Hadsell et al. [14] first proposed to learn representations by contrasting positive pairs against negative pairs.\u00bb\\n[2] \u00ab2203.03897v2.txt | A contrastive learning [4, 5] is a widely-used learning algorithm that effectively minimizes the distances between positive pairs while maximizing the dissimilarity between negative pairs. Due to its powerfulness in learning discriminative representation, contrastive learning has been actively adopted to pre-train the model and apply it for many computer vision applications.\u00bb\\n[3] \u00ab2208.13442v1.txt | For alleviating the above intractable issues, the representation learning for each task can be enhanced with unsupervised learning paradigm to explicitly capture and utilize the relation between tasks. Contrastive learning recently has proved to have very promising results in unsupervised representation learning (Lin et al. 2022). The objective of contrastive learning is to make all sample representations uniformly distributed while aligning the representations of similar samples (Wang and Liu 2021). To learning informative representations, the positive and negative samples for each task are constructed based on the underlying meaning. Motivated by these aspects, we suggest modeling the task relationship in a contrastive learning paradigm.\u00bb\\n[4] \u00ab2201.02732v2.txt | Contrastive learning is a widely adopted pre-training technique which learns the representations by pulling semantically close representations together and pushing apart non-related ones. In our setting, we have three representations eC , eG and eR depicting the same user preference from different views, which are considered as semantically close ones. Therefore, based on the three representations, we take the pairs (eC , eG ), (eC , eR ), and (eG , eR ) as positive examples, while the representations from different users in the same batch are considered as negative examples. Thus, for a mini-batch with b pairs, the coarse-grained pre-training objective is the sum of the triple contrast learning loss, which can be formulated as: Lcoarse = LCL (eC , eG ) + LCL (eC , eR ) + LCL (eG , eR\u00bb\\n[5] \u00ab2111.11294v5.txt | Related Work Contrastive Learning. Contrastive learning (Hadsell, Chopra, and LeCun 2006) aims to learn high-quality representations by contrasting positive sample pairs against negative sample pairs. Chen et al. (2020) and Caron et al. (2020) demonstrated that contrastive visual representation learning could produce comparable results to the supervised method on several vision tasks. These approaches have been popularized for other domains, such as language modeling (Gao, Yao, and Chen 2021) and speech recognition (Baevski et al. 2020). Recently, Akbari et al. (2021) and Radford et al. (2021) presented a method for learning a flexible prediction space for different modalities by employing contrastive losses to train the model.\u00bb\\n\\nQuestion: Contrastive Representational Learning\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}