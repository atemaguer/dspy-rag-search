{"duration": 1.6711678504943848, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\nQuestion: In what year was the star of To Hell and Back born?\\nAnswer: 1925\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00abtitle->Irony and Stereotype Spreaders Detection using BERT-large and AutoGulon | abstract->With the continuous development of the Internet, the Internet has become the mainstream way for people to socialize, and there is more and more content on the Internet. However, with the development of social networks comes the emergence of many Irony and stereotyped remarks, making the need for an automatic detection system more urgent. This paper provides a solution to the \"Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO)\" task proposed by PAN CLEF 2022, using BERT-large and AutoGluon to process and predict the data, and the final submitted score is 94.44 % .\u00bb\\n[2] \u00abtitle->SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction | abstract->With the rapid progress of AI in both academia and industry, Deep Learning has been widely introduced into various areas in drug discovery to accelerate its pace and cut R&D costs. Among all the problems in drug discovery, molecular property prediction has been one of the most important problems. Unlike general Deep Learning applications, the scale of labeled data is limited in molecular property prediction. To better solve this problem, Deep Learning methods have started focusing on how to utilize tremendous unlabeled data to improve the prediction performance on small-scale labeled data. In this paper, we propose a semi-supervised model named SMILES-BERT, which consists of attention mechanism based Transformer Layer. A large-scale unlabeled data has been used to pre-train the model through a Masked SMILES Recovery task. Then the pre-trained model could easily be generalized into different molecular property prediction tasks via fine-tuning. In the experiments, the proposed SMILES-BERT outperforms the state-of-the-art methods on all three datasets, showing the effectiveness of our unsupervised pre-training and great generalization capability of the pre-trained model.\u00bb\\n\\nQuestion: How many parameters does BERT large have?\\n\\nRationale: Let\\'s think step by step.', 'temperature': 0.0, 'top_p': 1}"}}