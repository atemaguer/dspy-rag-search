{"duration": 22.334203958511353, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab1908.10408v1.txt | The Query Encoder is the standard Transformer encoder, whose output token representations are subject to a projection to maintain the same model dimensionality across the levels. The Masked Session Encoder prevents information flow from subsequent queries. The shadow underneath the encoders conveys that they consist of several layers. Note that we add positional encodings (PE) for tokens in each query, and for queries in each session. The Add & Norm layer first computes the sum of its two arguments, one from the masked session encoder and the other via a residual or skip connection from the query encoder, and then performs layer normalization. The output codes thus obtained for each query are subsequently fed into a standard Transformer decoder (not shown in the figure) that treats the queries independently. The decoder of MTN is the same as in Transformer (i.e.\u00bb\\n[2] \u00ab2111.05937v1.txt | Figure 4: Dual Encoder or Two Tower architecture [42]. Query Encoder (also referred to as Query Tower) is used for encoding the question, Document Encoder (also referred to as Document Tower) is used to encode the documents. The encoders are usually BERT models. In ODQA retrieval models like DPR and ORQA [101, 112], only the [CLS] output vector of the BERT model outputs embedding is used as the embedding vectors and the other token embeddings are ignored. The similarity value is usually the dot product among the normalized vectors.\u00bb\\n[3] \u00ab2208.07652v1.txt | 3.2.1 Query Encoder. The query encoder is to map the input query q = {w 1, w 2, . . . , w |q | } into a compact vector that can capture its essential topics. Specifically, the encoder represents the query q as a series of hidden vectors, i.e., Hq = Encoder(w 1, w 2, . . . , w |q | ), where Hq denotes the query representation. fi fi fi 3.2.2 Identifier Decoder. The decoder is to generate a sequence of document identifiers of the relevant documents for each query. Note the document identifier can be defined in different ways, such as the unique title and url of the document. Specifically, the probability of generating the n-th token wm,n in the m-th document identifier tm is defined as, p (wm,n |w \u2264m,<n , q) = Decoder(w \u2264m,<n , Hq ).\u00bb\\n[4] \u00ab2003.00708v1.txt | Query Encoder: The query encoder generates a query level encoding Vqi for every qi \u2208 S. This is done by first representing the query qi using vector embeddings of corresponding words {w1 , . . . , wlq }, and then sequentially feeding them into a bidirectional LSTM (BiLSTM) [14]. As shown in Fig. 2(a), the query encoder takes each of these word representations as input to the BiLSTM at every encoding step and updates the hidden states based on the forward and Image Captions and Multitask Learning for Query Reformulations 5 Updating of the session encoding after seeing query in session max-pooling max-pooling max-pooling k transitions from (i-1) to i t=1 k=1 k=2 k=3 k=lq k=1 k=2 k=i-1 k=1 k=2 k=3 k=i-1 (b) Session Encoder (a) Query Encoder Fig. 2.\u00bb\\n[5] \u00ab2009.10791v1.txt | USE-QA: The USE-QA retriever (Yang et al., 2019b) has separate encoders for the query and document. The query encoder is a transformer-based model, producing a 512-dimension embedding as the query representation. The document encoder has a transformer-based model to encode the answer sentence and a Convolutional Neural Network (CNN)-based model to encode the context. A single 512-dimension embedding is produced as the document representation. Finally, the relevance score is computed as the dot product of the query embedding and the document embedding. USEQA is pre-trained on large scale retrieval tasks and, as used in (Ahmad et al., 2019), we do not fine-tune it in the retrieval tasks.\u00bb\\n\\nQuestion: Query encoder\\n\\nAnswer:', 'temperature': 0.0, 'top_p': 1}"}}