{"duration": 4.382679224014282, "input_args": {"**": "{'frequency_penalty': 0, 'logprobs': 5, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 20, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\nQuestion: In what year was the star of To Hell and Back born?\\nAnswer: 1925\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab2210.10584v1.txt | 2.7.2 Masked Language Modeling In contrast to the ICT task, (Guu et al., 2020) uses additional training task to achieve better results. They demonstrate for the first time how to pretrain such a generative knowledge retriever in an unsupervised manner using backpropagation through a retrieval phase that takes into account millions of documents and masked language modelling as the learning signal. Retrieval-Augmented Language Model (REALM central)\\'s tenet is to get the retriever trained using a signal from the unsupervised text which is performance-based: a retrieval that increases the perplexity of the LM and is rewarded is rewarded, while a retrieval that is uninformative is penalized. By using a latent variable LM and a retrieve-then-predict strategy, along with marginal likelihood optimization, this behaviour is obtained.\u00bb\\n[2] \u00ab2110.07752v2.txt | Retrieval for Language Modeling To improve the perplexity of a pre-trained language model, Khandelwal et al. (2020) retrieve similar contexts from the training set at each time-step and increase the likelihood of tokens that were predicted in similar contexts. Guu et al. (2020) instead pretrain a retrieval-augmented masked language model using salient-span masking and fine-tune it on downstream QA tasks. Under the paradigm of retrieval-augmented generation, for input x and output y, Lewis et al. (2020) retrieve top-k passages (z) from a corpus with a retriever and jointly train the generator (P\u03b8 ) and the retriever (P\u03b7 ) by maximizing\u00bb\\n[3] \u00ab2205.02355v1.txt | 4.2 Retrieval-based Language Models Recently, Retrieval-based language models (R-LMs) [1] have risen to improve over language models in various tasks such as unconditional language modeling [14, 25], machine translation [10, 24], text classification [27] and question answering [23]. R-LMs is a non-parametric method that uses training examples to augment the language model predictions at test time. The core ingredient is that they don\\'t have to only rely on the knowledge encoded in the model\\'s already-trained weights but use nearest neighbors to find augmented samples based on the untrained PLMs. Different from above methods, we initially propose to build an open-book datastore based on already-trained PLMs with prompt tuning. We are the first retrieval-enhanced prompt tuning approach for RE to the best of our knowledge.\u00bb\\n[4] \u00ab2106.05346v2.txt | Other application areas. In addition to question answering, retrieval-augmented methods have been successfully applied to other natural language processing tasks. In left-to-right language modeling, retrieving similar words from an external memory has been shown to improve perplexity (Khandelwal et al., 2020, Yogatama et al., 2021). In machine translation, retrieving domain-specific target language tokens has improved performance in domain adaptation (Khandelwal et al., 2021). Finally, in dialog modeling, retrieving knowledge-informed text has helped improve factual correctness in the generated conversations (Fan et al., 2021). We provide a detailed comparison of E MDR2 with some of the previous work in Appendix C and D. 5 Discussion Summary of contributions. We presented E MDR2 , an end-to-end training method for retrievalaugmented question answering systems. We showed how to arrive at our training objective using the expectation-maximization algorithm. We demonstrated that E MDR2 achieves state-of-the-art performance on three benchmark OpenQA datasets.\u00bb\\n[5] \u00ab2210.02627v1.txt | Recently, Retrieval Augmented Models (RAGs) have drawn considerable attention from researchers. RAG consists of a state-of-the-art-neural retriever called Dense Passage Retrieval (DPR) (Karpukhin et al., 2020) and BART seq2seq language model (Lewis et al., 2020a). Compared to the conventional two-staged ODQA pipelines, RAG merges the retriever and reader stages into one architecture. Moreover, unlike expensive language models with billions of parameters (e.g., GPT-3 (Brown et al., 2020) and MegatroneLM (Narayanan et al., 2021)) where the model\\'s parametric memory represents the complete knowledge, RAG can also extract knowledge from an external knowledge base. Using both parametric and non-parametric memory generally leads to reduced hallucinations and higher interpretability in tasks like question answering and summarization (Xu et al., 2021; Komeili et al., 2021; Guu et al., 2020; Lewis et al., 2020b).\u00bb\\n\\nQuestion: Retrieval Augmented Language Model(REALM)\\n\\nRationale: Let\\'s think step by step.', 'temperature': 0.7, 'top_p': 1}"}}