{"duration": 3.461926221847534, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2012.04682v2.txt | (2019) for predicting a target word given the context around the word. The RoBERTa transformer method is a pure application of MLM, removing the next sentence prediction task while scaling to longer sequences with dynamic masking. A crossentropy loss is used for prediction, the RoBERTa 50K byte-pair encoding tokenization is used, and hyper-parameters are left at default settings from Wolf et al. (2019). During training, the inputs are the CORD-19 dataset described in sec. 3.1 dynamically masked ten times. The MLM training\u00bb\\n[2] \u00ab2103.02462v1.txt | Training We use a large pretrained transformer language model as our stance detection model, namely Roberta [19] from the HuggingFace transformers library [34]. Roberta is a large transformer model pretrained on massive unlabelled text corpora using a masked language modeling objective. Models trained in this manner require only to be fine-tuned on downstream tasks, and have shown to achieve state of the art performance after doing so [11]. In this, we only require a labelled dataset on which to fine-tune the model. As no labelled dataset currently exists for the specific problem of health related stance detection within the Common Crawl News corpus, a large general domain dataset is used to train the stance detection model.\u00bb\\n[3] \u00ab2209.00470v1.txt | 3.3 RoBERTa RoBERTa (Y. Liu et al. 2019) is part of a family of language models built on top of the transformers architecture that allows for learning general contextual representations using self-supervised training. The variation in models such as BERT (Kenton and Toutanova 2019), XLM (Lample and Conneau 2019) and T5 (Raffel et al. 2020) primarily comes from differences in training objectives. A benefit of RoBERTa, BERT and similar transformer-based methods is that the context of a term can play an important role. With sequential models such as LSTMs long range inter-word dependencies are hardly taken into account for the simple reason that direct neighbors have more influence on eachother6 . Bidirectional LSTM's alleviate the uni-directionality but still suffer from the inability to incorporate large contexts.\u00bb\\n[4] \u00ab2208.05777v1.txt | Therefore, we employ a Transformer-based model as a wrapper for the bias recognition task. The Transformer-based model can also detect long-term context in data, allowing us to identify more biasbearing words in the text. We employ the RoBERTa [43], a retrained version of BERT with enhanced training methodology, and fine-tune it on our dataset. By including the Transformer in the standard NER pipeline, our bias recognition model can now identify many other biased words beyond the ones defined in the dataset. The final output from the bias recognition module is a set of news articles, where the biased words have been identified.\u00bb\\n[5] \u00ab2208.05777v1.txt | We also use the Transformer-based embeddings, such as from BERT, which are on dynamic word embeddings. Transformers are large encoder-decoder models that employ a sophisticated attention mechanism to process an entire sequence. The results show that Transformer-based methods outperform the other methods (ML and simple deep learningbased methods) in the bias detection task. Among the Transformer-based approaches, RoBERTa outperforms the BERT model by approximately 2% in terms of F1-score, while DistilBERT outperforms the RobBERTa model by approximately 3%.\u00bb\\n\\nQuestion: RoBERTa Transformer\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}