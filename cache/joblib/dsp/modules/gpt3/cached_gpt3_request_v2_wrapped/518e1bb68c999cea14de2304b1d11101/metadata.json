{"duration": 5.577390670776367, "input_args": {"**": "{'frequency_penalty': 0, 'logprobs': 5, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 20, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\nQuestion: In what year was the star of To Hell and Back born?\\nAnswer: 1925\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab2109.13984v1.txt | 4.1 Model Used For benchmarking Simple-SQuAD, we use two different variations of RoBERTa as introduced by Liu et al. (Liu et al., 2019). RoBERTa is a replication study of BERT pretraining, which is trained on more extensive training data with bigger batches, longer sequences, and dynamically changing masking patterns. Consequently, RoBERTa achieves better results over BERT and attains state-of-theart results on GLUE, RACE, and SQuAD. 4.2 Experimental Settings We perform a dataset-based ablation study, experimenting with multiple variants of input datasets for each model. Firstly, we finetune the model on the SQuAD and the Simple-SQuAD dataset separately for 2 epochs. We then finetune the SimpleSQuAD trained model on the SQuAD dataset and the SQuAD-trained model on the Simple-SQuAD dataset for 2 epochs each. We benchmark the results for each of these combinations of the dataset input to better infer the effect of simplifying sentences in the original dataset.\u00bb\\n[2] \u00ab2209.00470v1.txt | 3.3 RoBERTa RoBERTa (Y. Liu et al. 2019) is part of a family of language models built on top of the transformers architecture that allows for learning general contextual representations using self-supervised training. The variation in models such as BERT (Kenton and Toutanova 2019), XLM (Lample and Conneau 2019) and T5 (Raffel et al. 2020) primarily comes from differences in training objectives. A benefit of RoBERTa, BERT and similar transformer-based methods is that the context of a term can play an important role. With sequential models such as LSTMs long range inter-word dependencies are hardly taken into account for the simple reason that direct neighbors have more influence on eachother6 . Bidirectional LSTM\\'s alleviate the uni-directionality but still suffer from the inability to incorporate large contexts.\u00bb\\n[3] \u00ab2009.06375v3.txt | 3.2.1 RoBERTa The meaning of words vary subtly across different contexts, and RoBERTa generates contextualized word representations to capture the contextsensitive semantics of words (Liu et al., 2019). The use of word representations from RoBERTa results in the state-of-the-art performance in a wide variety of language understanding tasks. Given a sentence s consisting of n words {w1 , . . . , wn }, RoBERTa model generates their contextualized representations {vcs,w1 , . . . , vcs,wn }. 3.2.2 XLNet XLNet is an auto-regressive language model which is based on the transformer architecture with recurrence (Yang et al., 2019). It outputs the joint probability of a sequence of tokens. The training objective calculates the probability of a word token conditioned on all permutations of word tokens in a sentence, as opposed to just those to the left or to the right of the target token.\u00bb\\n[4] \u00ab2206.03474v1.txt | COVID-QA system (M\u00f6ller et al. 2020): COVID-QA is a question answering system based on the Robustly optimized BERT approach (RoBERTa) model (Liu et al. 2019). RoBERTa is the retraining of BERT with improved training methodology, more data and better computations. We report the results for each baseline according to its optimal hyperparameter setting and report the best results for each baseline. We have divided the COVID-19 dataset into training, validation, and test sets, with a 75:15:15 ratio for all experiments. We evaluate the results using CQuAD, COVID-QA and BioASQ evaluation datasets. 5.3. Evaluation Metrics In this work, we make use of the following evaluation metrics: - Precision (prec), Recall and Mean Reciprocal Rank (MRR) to evaluate retriever. - Accuracy (acc), Exact Match (EM) and Semantic Answer Similarity (SAS) to evaluate reader. Precision is the fraction of retrieved documents that are relevant (Teufel 2007).\u00bb\\n[5] \u00ab2010.08652v1.txt | BERT (Devlin et al., 2019) is a language representation model based on a multi-layer bidirectional Transformer encoder architecture. It uses a masked language model objective and a next sentence prediction objective during pre-training. RoBERTa (Liu et al., 2019) improves the pretraining of BERT by training the model longer with bigger batches and more data. Our work uses the multilingual version of BERT, named mBERT1 , and the multilingual version of RoBERTa, named XLM-R (Conneau et al., 2020). mBERT was pre-trained with Wikipedia text of 104 languages with the largest sizes, and XLM-R were pre-trained with Wikipedia text and CommonCrawl Corpus of 100 languages. Both models use no cross-lingual resources and belong to the unsupervised representation learning framework.\u00bb\\n[6] \u00ab2012.04682v2.txt | (2019) for predicting a target word given the context around the word. The RoBERTa transformer method is a pure application of MLM, removing the next sentence prediction task while scaling to longer sequences with dynamic masking. A crossentropy loss is used for prediction, the RoBERTa 50K byte-pair encoding tokenization is used, and hyper-parameters are left at default settings from Wolf et al. (2019). During training, the inputs are the CORD-19 dataset described in sec. 3.1 dynamically masked ten times. The MLM training\u00bb\\n[7] \u00ab2103.02462v1.txt | Training We use a large pretrained transformer language model as our stance detection model, namely Roberta [19] from the HuggingFace transformers library [34]. Roberta is a large transformer model pretrained on massive unlabelled text corpora using a masked language modeling objective. Models trained in this manner require only to be fine-tuned on downstream tasks, and have shown to achieve state of the art performance after doing so [11]. In this, we only require a labelled dataset on which to fine-tune the model. As no labelled dataset currently exists for the specific problem of health related stance detection within the Common Crawl News corpus, a large general domain dataset is used to train the stance detection model.\u00bb\\n[8] \u00ab2208.05777v1.txt | Therefore, we employ a Transformer-based model as a wrapper for the bias recognition task. The Transformer-based model can also detect long-term context in data, allowing us to identify more biasbearing words in the text. We employ the RoBERTa [43], a retrained version of BERT with enhanced training methodology, and fine-tune it on our dataset. By including the Transformer in the standard NER pipeline, our bias recognition model can now identify many other biased words beyond the ones defined in the dataset. The final output from the bias recognition module is a set of news articles, where the biased words have been identified.\u00bb\\n[9] \u00ab2204.06758v1.txt | A transformer model is a deep learning model with self-attention mechanisms (5). The original transformer model was a sequence-to-sequence model and greatly improved the performance of machine translation (5). Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based model and is pretrained on two tasks: masked language modeling and next sentence prediction (6). Thanks to this pretraining process, BERT learns contextual embeddings of words. BERT and its variants (e.g. RoBERTa (7)) have brought significant performance gains on a variety of language tasks. BERT has been adapted to the biomedical domain (8-10). Recently, we pretrained a compact biomedical BERT model named Bioformer. In this study, we focus on solving the multi-label topic classification problem using Bioformer and comparing its performance with other two biomedical BERT models (BioBERT (8) and PubMedBERT (10)).\u00bb\\n\\nQuestion: RoBERTa Transformer Information Retrieval\\n\\nRationale: Let\\'s think step by step.', 'temperature': 0.7, 'top_p': 1}"}}