{"duration": 1.7590842247009277, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00abtitle->Irony and Stereotype Spreaders Detection using BERT-large and AutoGulon | abstract->With the continuous development of the Internet, the Internet has become the mainstream way for people to socialize, and there is more and more content on the Internet. However, with the development of social networks comes the emergence of many Irony and stereotyped remarks, making the need for an automatic detection system more urgent. This paper provides a solution to the \"Profiling Irony and Stereotype Spreaders on Twitter (IROSTEREO)\" task proposed by PAN CLEF 2022, using BERT-large and AutoGluon to process and predict the data, and the final submitted score is 94.44 % .\u00bb\\n[2] \u00abtitle->SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction | abstract->With the rapid progress of AI in both academia and industry, Deep Learning has been widely introduced into various areas in drug discovery to accelerate its pace and cut R&D costs. Among all the problems in drug discovery, molecular property prediction has been one of the most important problems. Unlike general Deep Learning applications, the scale of labeled data is limited in molecular property prediction. To better solve this problem, Deep Learning methods have started focusing on how to utilize tremendous unlabeled data to improve the prediction performance on small-scale labeled data. In this paper, we propose a semi-supervised model named SMILES-BERT, which consists of attention mechanism based Transformer Layer. A large-scale unlabeled data has been used to pre-train the model through a Masked SMILES Recovery task. Then the pre-trained model could easily be generalized into different molecular property prediction tasks via fine-tuning. In the experiments, the proposed SMILES-BERT outperforms the state-of-the-art methods on all three datasets, showing the effectiveness of our unsupervised pre-training and great generalization capability of the pre-trained model.\u00bb\\n\\nQuestion: How many parameters does BERT large have?\\n\\nRationale: Let\\'s think step by step. Based on the context, we have learned the following.', 'temperature': 0.0, 'top_p': 1}"}}