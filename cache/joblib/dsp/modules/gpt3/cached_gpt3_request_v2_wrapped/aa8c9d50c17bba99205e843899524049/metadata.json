{"duration": 1.4955017566680908, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2109.13984v1.txt | 4.1 Model Used For benchmarking Simple-SQuAD, we use two different variations of RoBERTa as introduced by Liu et al. (Liu et al., 2019). RoBERTa is a replication study of BERT pretraining, which is trained on more extensive training data with bigger batches, longer sequences, and dynamically changing masking patterns. Consequently, RoBERTa achieves better results over BERT and attains state-of-theart results on GLUE, RACE, and SQuAD. 4.2 Experimental Settings We perform a dataset-based ablation study, experimenting with multiple variants of input datasets for each model. Firstly, we finetune the model on the SQuAD and the Simple-SQuAD dataset separately for 2 epochs. We then finetune the SimpleSQuAD trained model on the SQuAD dataset and the SQuAD-trained model on the Simple-SQuAD dataset for 2 epochs each. We benchmark the results for each of these combinations of the dataset input to better infer the effect of simplifying sentences in the original dataset.\u00bb\\n[2] \u00ab2209.00470v1.txt | 3.3 RoBERTa RoBERTa (Y. Liu et al. 2019) is part of a family of language models built on top of the transformers architecture that allows for learning general contextual representations using self-supervised training. The variation in models such as BERT (Kenton and Toutanova 2019), XLM (Lample and Conneau 2019) and T5 (Raffel et al. 2020) primarily comes from differences in training objectives. A benefit of RoBERTa, BERT and similar transformer-based methods is that the context of a term can play an important role. With sequential models such as LSTMs long range inter-word dependencies are hardly taken into account for the simple reason that direct neighbors have more influence on eachother6 . Bidirectional LSTM's alleviate the uni-directionality but still suffer from the inability to incorporate large contexts.\u00bb\\n[3] \u00ab2009.06375v3.txt | 3.2.1 RoBERTa The meaning of words vary subtly across different contexts, and RoBERTa generates contextualized word representations to capture the contextsensitive semantics of words (Liu et al., 2019). The use of word representations from RoBERTa results in the state-of-the-art performance in a wide variety of language understanding tasks. Given a sentence s consisting of n words {w1 , . . . , wn }, RoBERTa model generates their contextualized representations {vcs,w1 , . . . , vcs,wn }. 3.2.2 XLNet XLNet is an auto-regressive language model which is based on the transformer architecture with recurrence (Yang et al., 2019). It outputs the joint probability of a sequence of tokens. The training objective calculates the probability of a word token conditioned on all permutations of word tokens in a sentence, as opposed to just those to the left or to the right of the target token.\u00bb\\n[4] \u00ab2206.03474v1.txt | COVID-QA system (M\u00f6ller et al. 2020): COVID-QA is a question answering system based on the Robustly optimized BERT approach (RoBERTa) model (Liu et al. 2019). RoBERTa is the retraining of BERT with improved training methodology, more data and better computations. We report the results for each baseline according to its optimal hyperparameter setting and report the best results for each baseline. We have divided the COVID-19 dataset into training, validation, and test sets, with a 75:15:15 ratio for all experiments. We evaluate the results using CQuAD, COVID-QA and BioASQ evaluation datasets. 5.3. Evaluation Metrics In this work, we make use of the following evaluation metrics: - Precision (prec), Recall and Mean Reciprocal Rank (MRR) to evaluate retriever. - Accuracy (acc), Exact Match (EM) and Semantic Answer Similarity (SAS) to evaluate reader. Precision is the fraction of retrieved documents that are relevant (Teufel 2007).\u00bb\\n[5] \u00ab2010.08652v1.txt | BERT (Devlin et al., 2019) is a language representation model based on a multi-layer bidirectional Transformer encoder architecture. It uses a masked language model objective and a next sentence prediction objective during pre-training. RoBERTa (Liu et al., 2019) improves the pretraining of BERT by training the model longer with bigger batches and more data. Our work uses the multilingual version of BERT, named mBERT1 , and the multilingual version of RoBERTa, named XLM-R (Conneau et al., 2020). mBERT was pre-trained with Wikipedia text of 104 languages with the largest sizes, and XLM-R were pre-trained with Wikipedia text and CommonCrawl Corpus of 100 languages. Both models use no cross-lingual resources and belong to the unsupervised representation learning framework.\u00bb\\n\\nQuestion: RoBERTa Transformer Information Retrieval\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}