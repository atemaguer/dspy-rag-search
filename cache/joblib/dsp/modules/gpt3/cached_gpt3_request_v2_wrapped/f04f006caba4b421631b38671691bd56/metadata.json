{"duration": 3.970106840133667, "input_args": {"**": "{'frequency_penalty': 0, 'logprobs': 5, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 20, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\nQuestion: In what year was the star of To Hell and Back born?\\nAnswer: 1925\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab2110.11570v2.txt | Contrastive Learning Contrastive Learning is a framework to learn representations that obey similarity constraints in a dataset typically organized by similar and dissimilar pairs. Hadsell et al. [14] first proposed to learn representations by contrasting positive pairs against negative pairs.\u00bb\\n[2] \u00ab2203.03897v2.txt | A contrastive learning [4, 5] is a widely-used learning algorithm that effectively minimizes the distances between positive pairs while maximizing the dissimilarity between negative pairs. Due to its powerfulness in learning discriminative representation, contrastive learning has been actively adopted to pre-train the model and apply it for many computer vision applications.\u00bb\\n[3] \u00ab2208.13442v1.txt | For alleviating the above intractable issues, the representation learning for each task can be enhanced with unsupervised learning paradigm to explicitly capture and utilize the relation between tasks. Contrastive learning recently has proved to have very promising results in unsupervised representation learning (Lin et al. 2022). The objective of contrastive learning is to make all sample representations uniformly distributed while aligning the representations of similar samples (Wang and Liu 2021). To learning informative representations, the positive and negative samples for each task are constructed based on the underlying meaning. Motivated by these aspects, we suggest modeling the task relationship in a contrastive learning paradigm.\u00bb\\n[4] \u00ab2201.02732v2.txt | Contrastive learning is a widely adopted pre-training technique which learns the representations by pulling semantically close representations together and pushing apart non-related ones. In our setting, we have three representations eC , eG and eR depicting the same user preference from different views, which are considered as semantically close ones. Therefore, based on the three representations, we take the pairs (eC , eG ), (eC , eR ), and (eG , eR ) as positive examples, while the representations from different users in the same batch are considered as negative examples. Thus, for a mini-batch with b pairs, the coarse-grained pre-training objective is the sum of the triple contrast learning loss, which can be formulated as: Lcoarse = LCL (eC , eG ) + LCL (eC , eR ) + LCL (eG , eR\u00bb\\n[5] \u00ab2111.11294v5.txt | Related Work Contrastive Learning. Contrastive learning (Hadsell, Chopra, and LeCun 2006) aims to learn high-quality representations by contrasting positive sample pairs against negative sample pairs. Chen et al. (2020) and Caron et al. (2020) demonstrated that contrastive visual representation learning could produce comparable results to the supervised method on several vision tasks. These approaches have been popularized for other domains, such as language modeling (Gao, Yao, and Chen 2021) and speech recognition (Baevski et al. 2020). Recently, Akbari et al. (2021) and Radford et al. (2021) presented a method for learning a flexible prediction space for different modalities by employing contrastive losses to train the model.\u00bb\\n[6] \u00ab2105.06247v1.txt | Contrastive learning. Contrastive learning (CL) usually serves as an unsupervised objective to learn representations by contrasting positive pairs against negative pairs [12, 22, 25, 49, 71, 86]. In our context, a positive pair is a matching video-query pair, and a negative pair is a non-matching video and query. One way to achieve contrastive learning is to directly maximize the mutual information (MI) [5, 33] between latent representations [2, 31]. There are also solutions to estimate the\u00bb\\n[7] \u00ab2208.08024v1.txt | Recently, the contrastive learning objective has become advantageous for unsupervised representation learning and led to stateof-the-art results in various domains. The essence of contrastive learning lies in the infomax [17] principle, where mutual information of a query and its positive views should be maximized, and the strategy of obtaining positive pairs. Typical augmentation methods for computer vision [2] include random cropping and flipping [23]. Tian et al., [27] also use different views of the same scene as positive samples. Similarly, in natural language processing, Logeswaran and Lee [21] use the context sentences\u00bb\\n[8] \u00ab2204.02119v1.txt | Contrastive learning. Contrastive learning has been widely used in SBR, which can be used as an auxiliary task to enhance the performance of model. It is expected to better characterize different aspects of sessions by contrasting two groups of session embeddings learned via two views (intersession and intra-session). Thus, we next present how to learn contrast objective to enhance the performance in characterizing session feature.\u00bb\\n[9] \u00ab2203.03897v2.txt | 2.1 Contrastive Learning Contrastive learning has been widely utilized for unsupervied representation learning or a pretraining on various domains [1, 12, 13, 14]. The goal of contrastive learning is to learn an embedding function that maps data into an embedding space so that semantically similar data have close embeddings under some distance metric. While any distance metric can be used, Cosine distance [1], or Euclidian distance [4] metric is usually used for simplicity.\u00bb\\n\\nQuestion: Contrastive Representational Learning\\n\\nRationale: Let\\'s think step by step.', 'temperature': 0.7, 'top_p': 1}"}}