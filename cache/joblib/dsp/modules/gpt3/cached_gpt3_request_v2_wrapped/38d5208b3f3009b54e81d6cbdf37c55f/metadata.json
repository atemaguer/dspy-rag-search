{"duration": 8.471907138824463, "input_args": {"**": "{'frequency_penalty': 0, 'logprobs': 5, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 20, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\nQuestion: In what year was the star of To Hell and Back born?\\nAnswer: 1925\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let\\'s think step by step. ${a step-by-step deduction that identifies the correct response, which will be provided below}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab2212.08841v1.txt | References Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999. Modern information retrieval, volume 463. ACM press New York. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144. Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2019. Pre-training tasks for embedding-based large-scale retrieval. In International Conference on Learning Representations. Xilun Chen, Kushal Lakhotia, Barlas O\u011fuz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. 2021. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918. Sukmin Cho, Soyeong Jeong, Wonsuk Yang, and Jong C Park. 2022. Query generation with external knowledge for dense retrieval. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 22\u201332.\u00bb\\n[2] \u00ab2210.15718v1.txt | 7 Conclusion This paper provides a practical recipe for combining Retrieval Augmentation and Large Language Models. In particular, we proposed QUILL as an approach to tackle the problem of query intent classification. Our empirical study demonstrates conclusively that Retrieval Augmentation can provide significant value over existing approaches. Furthermore we show that via our two-stage distillation approach, that QUILL not only learns better performing, more robust teachers, but also leads to even bigger gains when distilled into fast, realworld capable production student models. 8 Acknowledgements We sincerely thank Jiecao Chen, William Dennis Kunz, Austin Tarango, Lee Gardner, Yang Zhang, Constance Wang, Derya Ozkan, Nitin Nalin, Raphael Hoffmann, Iftekhar Naim, Siddhartha Brahma, Siamak Shakeri, Hongkun Yu, John Nham, Ming-Wei Chang, Marc Najork, Corinna Cortes and many others for their insightful feedback and help. We also thank the EMNLP Reviewers for their thorough review, feedback and suggestions.\u00bb\\n[3] \u00ab2201.10582v1.txt | Deep LM augmented lexical retriever More recently, pre-trained deep language models (LM) such as BERT [10] have been shown to be powerful in many natural language understanding tasks. The very first application of such models in IR is to augment lexical retrieval models. Dai et al. [7,9] proposed to learn context-aware term weights by BERT to replace the term frequencies used by lexical models. To remedy the vocabulary gap between queries and documents, Nogueira and Lin [29,28] employed seq2seq model transformer [39] and later T5 [33] to generate document expansions, which brings significant gains for BM25. In the same vein, Mao et al. [27] adopted seq2seq model BART [20] to generate query expansions, which outperforms RM3 [15], a highly performant lexical query expansion method.\u00bb\\n[4] \u00ab2205.01230v1.txt | In the context of language modeling, one class of methods uses retrieval results as evidence to support reasoning. For example, the knowledge retriever module in REALM [21] accesses information from an encoded Wikipedia corpus during pre-training. In text generation, RetGen [75] combines a grounded text generator with a document retriever. Grounding the generation helps with the issue of hallucinated facts, and the retrieval component makes the grounding effective and efficient. Lewis et al. [39] highlighted the importance of retrieval in knowledge-intensive NLP tasks and introduced retrieval-augmented generation (RAG) by augmenting a generator with the output of a non-parametric retriever that uses maximum inner product search. Entities as Experts (EaE) [15] introduces an entity memory that can be accessed by the model and the retrieved representations of entities are combined with the input representation for entity linking, mention detection, and masked language modeling tasks.\u00bb\\n[5] \u00ab2211.14876v1.txt | As a representative work, REALM [94] utilizes a knowledge retriever for retrieving relevant texts from a large background corpus, and the retrieved texts are further encoded and attended to augment language model pretraining. In REALM, the basic idea is to reward or discourage the retriever according to whether the retrieved context is useful to improve the prediction of the masksed words. Without using explicit human annotation, the context retriever is further trained via language modeling pretraining, in which the retrieved texts are modeled by a latent variable through marginal likelihood. By fine-tuning the model, REALM performs well on the task of open-domain question answering. It further proposes to use an asynchronous optimization approach based on the maximum inner product search. As an extension work, Balachandran et al. [236] perform a thorough tuning of REALM on a variety of QA tasks.\u00bb\\n[6] \u00ab2109.01628v1.txt | 4.1 Model-based Transfer By exploiting the zero-shot cross-lingual transfer ability of pre-trained transformers such as mBERT (Devlin et al., 2018) and XLMRoberta (Conneau et al., 2019), we train the dense retriever encoder in the source language and apply inference directly on target languages. These pre-trained transformers only require raw text in different languages, e.g. Wikipedia, and are trained in a self-supervised manner, so we characterize this approach as \"low resource\". 4.2 Target Language Transfer To bridge the language gap between the training and the inference, a direct solution is target language data augmentation. In this work, we explore two techniques for creating a target language transfer set, including generation-based query synthesis and weakly supervised query synthesis.\u00bb\\n[7] \u00ab2210.05758v1.txt | 5 5.1 Open Domain Question Answering Natural Question Large language models are useful because they generalize to downstream tasks, in addition to performing auto-regressive language generation. To demonstrate that the proposed context augmentation scheme is effective on downstream applications we evaluated our proposed model of decoupled Encoder-Decoder on the OpenQA task of Natural Question [23]. We use the same question and context processed by [18], where the context is retrieved with DPR retriever [20]. We construct each context sequence in the format of \"title: {title} source: {source}\" and pad or trim it to 256 sentencepiece tokens. The context sequences are then encoded by the encoder independently and simply concatenated, to ensure there is no interaction between them.\u00bb\\n\\nQuestion: What is the title of the paper that introduced the paradigm of retriever augmentation of language models(LM)\\n\\nRationale: Let\\'s think step by step.', 'temperature': 0.7, 'top_p': 1}"}}