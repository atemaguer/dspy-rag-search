{"duration": 1.3315198421478271, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': 'Answer questions with short factoid answers.\\n\\n---\\n\\nQuestion: Which award did the first book of Gary Zukav receive?\\nAnswer: U.S. National Book Award\\n\\nQuestion: The heir to the Du Pont family fortune sponsored what wrestling team?\\nAnswer: Foxcatcher\\n\\nQuestion: Who was the director of the 2009 movie featuring Peter Outerbridge as William Easton?\\nAnswer: Kevin Greutert\\n\\nQuestion: Who produced the album that included a re-recording of \"Lithium\"?\\nAnswer: Butch Vig\\n\\nQuestion: What city was the victim of Joseph Druces working in?\\nAnswer: Boston, Massachusetts\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nAnswer: ${a short factoid answer, often between 1 and 5 words}\\n\\n---\\n\\nContext:\\n[1] \u00ab2210.13578v1.txt | Keywords: Inverted Index * Question Answering * Large Language Model. 1 Introduction and background Advances in large language models (LLMs), especially employing the transformer architecture [5], revolutionized the quality of natural language processing applications. However, training an LLM with state-of-the-art architecture, including several layers of decoder/encoder and attention, is computationally very expensive and not possible by small and medium enterprises with limited budgets. There are promising works to reduce the model sizes of LLMs such as compression [9,19], quantization [20] or the use of knowledge distillation [11], but still, larger models are favored due to their accuracy.\u00bb\\n[2] \u00ab2207.12515v1.txt | \u2022 Big Models: Recently, Foundation Models such as Large Language Models (LLM) have achieved surprisingly good performance in many AI sub-fields, which have the advantages of emergent capabilities from model size, extracting useful information based on self-supervision, unifying various downstream tasks based on pre-training, fine-tuning and prompting, as well as generalizing to zero-shot or few-shot cases [31, 290]. Many powerful foundation models have been developed for natural language tasks such as T5 [231], GPT-3 [35], OPT [335] and PaLM [69], which show impressive performance on language understanding, generation and reasoning tasks.\u00bb\\n[3] \u00ab2210.06345v1.txt | There is a growing interest in scaling Transformer-based language models to using larger datasets and a gargantuan number of parameters. Scaling such models has resulted in sustained returns on many downstream tasks.1 When applied to new tasks, large language models (LLMs) exploit the knowledge that was implicitly retained in their weights during training. However, implicit encoding of knowledge might be a factor that limits performances because i) the storage capacity of the model is bounded by the number of parameters, ii) controlling for the quality of the embedded knowledge is challenging iii) adapting to information that was not known at training time might require further pre-training.\u00bb\\n[4] \u00ab2210.15718v1.txt | 2 Related Work Large language models (LLMs) such as mT5 (Xue et al., 2021) demonstrated significant performance improvements on a variety of natural language understanding (NLU) tasks. Specifically in the context of query understanding, researchers found that (a) model size significantly effects the quality of the resulting models (Nogueira et al., 2019; Han et al., 2020), and (b) using additional context in the form of query-associated documents is crucial to the model performance due to the paucity of context available in the query itself (Nogueira and Lin, 2019; Zhang et al., 2020). Retrieval augmentation of the query with the search results retrieved by it is a proven way to incorporate such context in LLM training for NLU tasks, as has been shown recently by models such as RAG (Lewis et al., 2020), REALM (Guu et al., 2020), and RETRO (Borgeaud et al., 2022).\u00bb\\n[5] \u00ab2210.15718v1.txt | 1 Introduction The recent advent of billion+ parameter Large Language Models (LLMs) \u2013 such as T5 (Raffel et al., 2019), mT5 (Xue et al., 2021), GPT-3 (Brown et al., 2020) and most recently PaLM (Chowdhery et al., 2022) \u2013 has disrupted many language understanding tasks \u2013 with new benchmarks set or eclipsed routinely by these Transformer models and their variants. Queries \u2013 especially keyword search ones \u2013 present a unique challenge though. Their short \u2217 Corresponding Authors Mike Bendersky Google Research bemike@google.com length, inherent ambiguity and lack of grammar mean query understanding tasks typically require more memorization and world knowledge than other NLP tasks (Broder et al., 2007). Consequently, despite LLMs leading performance on language and query understanding tasks \u2013 like intent classification, query parsing and relevance prediction \u2013 there is significant room for further improvement.\u00bb\\n\\nQuestion: Large Language Model (LLM)\\n\\nAnswer:', 'temperature': 0.0, 'top_p': 1}"}}