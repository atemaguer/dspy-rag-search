{"duration": 2.27563214302063, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2212.08841v1.txt | References Ricardo Baeza-Yates, Berthier Ribeiro-Neto, et al. 1999. Modern information retrieval, volume 463. ACM press New York. Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144. Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. 2019. Pre-training tasks for embedding-based large-scale retrieval. In International Conference on Learning Representations. Xilun Chen, Kushal Lakhotia, Barlas O\u011fuz, Anchit Gupta, Patrick Lewis, Stan Peshterliev, Yashar Mehdad, Sonal Gupta, and Wen-tau Yih. 2021. Salient phrase aware dense retrieval: Can a dense retriever imitate a sparse one? arXiv preprint arXiv:2110.06918. Sukmin Cho, Soyeong Jeong, Wonsuk Yang, and Jong C Park. 2022. Query generation with external knowledge for dense retrieval. In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 22\u201332.\u00bb\\n[2] \u00ab2210.15718v1.txt | 7 Conclusion This paper provides a practical recipe for combining Retrieval Augmentation and Large Language Models. In particular, we proposed QUILL as an approach to tackle the problem of query intent classification. Our empirical study demonstrates conclusively that Retrieval Augmentation can provide significant value over existing approaches. Furthermore we show that via our two-stage distillation approach, that QUILL not only learns better performing, more robust teachers, but also leads to even bigger gains when distilled into fast, realworld capable production student models. 8 Acknowledgements We sincerely thank Jiecao Chen, William Dennis Kunz, Austin Tarango, Lee Gardner, Yang Zhang, Constance Wang, Derya Ozkan, Nitin Nalin, Raphael Hoffmann, Iftekhar Naim, Siddhartha Brahma, Siamak Shakeri, Hongkun Yu, John Nham, Ming-Wei Chang, Marc Najork, Corinna Cortes and many others for their insightful feedback and help. We also thank the EMNLP Reviewers for their thorough review, feedback and suggestions.\u00bb\\n[3] \u00ab2201.10582v1.txt | Deep LM augmented lexical retriever More recently, pre-trained deep language models (LM) such as BERT [10] have been shown to be powerful in many natural language understanding tasks. The very first application of such models in IR is to augment lexical retrieval models. Dai et al. [7,9] proposed to learn context-aware term weights by BERT to replace the term frequencies used by lexical models. To remedy the vocabulary gap between queries and documents, Nogueira and Lin [29,28] employed seq2seq model transformer [39] and later T5 [33] to generate document expansions, which brings significant gains for BM25. In the same vein, Mao et al. [27] adopted seq2seq model BART [20] to generate query expansions, which outperforms RM3 [15], a highly performant lexical query expansion method.\u00bb\\n[4] \u00ab2205.01230v1.txt | In the context of language modeling, one class of methods uses retrieval results as evidence to support reasoning. For example, the knowledge retriever module in REALM [21] accesses information from an encoded Wikipedia corpus during pre-training. In text generation, RetGen [75] combines a grounded text generator with a document retriever. Grounding the generation helps with the issue of hallucinated facts, and the retrieval component makes the grounding effective and efficient. Lewis et al. [39] highlighted the importance of retrieval in knowledge-intensive NLP tasks and introduced retrieval-augmented generation (RAG) by augmenting a generator with the output of a non-parametric retriever that uses maximum inner product search. Entities as Experts (EaE) [15] introduces an entity memory that can be accessed by the model and the retrieved representations of entities are combined with the input representation for entity linking, mention detection, and masked language modeling tasks.\u00bb\\n[5] \u00ab2211.14876v1.txt | As a representative work, REALM [94] utilizes a knowledge retriever for retrieving relevant texts from a large background corpus, and the retrieved texts are further encoded and attended to augment language model pretraining. In REALM, the basic idea is to reward or discourage the retriever according to whether the retrieved context is useful to improve the prediction of the masksed words. Without using explicit human annotation, the context retriever is further trained via language modeling pretraining, in which the retrieved texts are modeled by a latent variable through marginal likelihood. By fine-tuning the model, REALM performs well on the task of open-domain question answering. It further proposes to use an asynchronous optimization approach based on the maximum inner product search. As an extension work, Balachandran et al. [236] perform a thorough tuning of REALM on a variety of QA tasks.\u00bb\\n\\nQuestion: What is the title of the paper that introduced the paradigm of retriever augmentation of language models(LM)\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}