{"duration": 3.2941081523895264, "input_args": {"**": "{'frequency_penalty': 0, 'max_tokens': 150, 'model': 'text-davinci-002', 'n': 1, 'presence_penalty': 0, 'prompt': \"Write a search query that will help answer a complex question.\\n\\n---\\n\\nFollow the following format.\\n\\nContext:\\n${sources that may contain relevant content}\\n\\nQuestion: ${the question to be answered}\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following. ${information from the context that provides useful clues}\\n\\nSearch Query: ${a simple question for seeking the missing information}\\n\\n---\\n\\nContext:\\n[1] \u00ab2212.14024v1.txt | Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Rationale-augmented ensembles in language models. arXiv preprint arXiv:2207.00747, 2022b. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022c. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022. Wiher, G., Meister, C., and Cotterell, R. On decoding strategies for neural text generators. arXiv preprint arXiv:2203.15721, 2022. Xiong, W., Li, X. L., Iyer, S., Du, J., Lewis, P., Wang, W. Y., Mehdad, Y., Yih, W.-t., Riedel, S., Kiela, D., et al. Answering complex open-domain questions with multihop dense retrieval. arXiv preprint arXiv:2009.12756, 2020. URL https://arxiv.org/abs/2009.12756.\u00bb\\n[2] \u00ab2212.09597v1.txt | Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022a. Emergent abilities of large language models. CoRR, abs/2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, December 6-14, 2022, virtual. Peifeng Wang, Aaron Chan, Filip Ilievski, Muhao Chen, and Xiang Ren. 2022c. PINTO: faithful language reasoning using prompt-generated rationales. CoRR, abs/2211.01562.\u00bb\\n[3] \u00ab2210.05075v1.txt | Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. ArXiv, abs/2206.07682, 2022a. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pp. 38\u201345, 2020.\u00bb\\n[4] \u00ab2210.05075v1.txt | This paper introduces reflection of thought, a new idea of eliciting the numerical reasoning knowledge hidden in pre-trained LMs through probing with simple anchor numbers. Reflection of thought, in principle, allows models to unveil the underlying reasoning process by varying inputs at test time, so it does not need any extra training or labeled data. To inversely elicit arithmetic relationships in LMs through anchor numbers, we propose S O L I S, a novel method to transform and formulate this problem to\u00bb\\n[5] \u00ab1701.02163v1.txt | Valentina Franzoni and Alfredo Milani. 2016. A semantic comparison of clustering algorithms for the evaluation of web-based similarity measures, LNCS Valentina Franzoni and Alfredo Milani. 2014. Heuristic semantic walk for concept chaining in collaborative networks. Int. J. Web Inf. Syst. 10, 1 (2014). DOI:https://doi.org/10.1108/IJWIS-112013-0031 C.H.C. Leung, Y. Li, A. Milani, and V. Franzoni. 2013. Collective evolutionary concept distance based query expansion for effective web document retrieval, LNCS S. Pallottelli, V. Franzoni, and A. Milani. 2016. Multi-path traces in semantic graphs for latent knowledge elicitation. In Proceedings - International Conference on Natural Computation. DOI:https://doi.org/10.1109/ICNC.2015.7378004 Peter D. Turney. 2002. Mining the Web for Synonyms: {PMI-IR} versus {LSA} on {TOEFL}. CoRR cs.LG/0212 (2002). Author copy .\u00bb\\n\\nQuestion: Chain of Thought Elicits\\n\\nRationale: Let's think step by step. Based on the context, we have learned the following.\", 'temperature': 0.0, 'top_p': 1}"}}